{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge Scratchbook\n",
    "\n",
    "* This notebook explores methods for the Kernel Methods for Machine Learning Kaggle [challenge](https://www.kaggle.com/c/kernel-methods-for-machine-learning-2018-2019/data).\n",
    "\n",
    "* Note that this is a binary classification challenge.\n",
    "\n",
    "Our first goal is to implement two baseline methods:\n",
    "1. Random classification\n",
    "2. All instances are 0s (Doing so we get an idea of the proportion of 0's in the public test set)\n",
    "3. Implement the Simple Pattern Recognition Algorithm (SPR) from Learning with Kernels \n",
    "\n",
    "Before that, we have to implement some data loaders\n",
    "\n",
    "\n",
    "Now that we are done with the above, our goal is to implement SVM with Gaussian kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import numpy as np\n",
    "from scipy import optimize\n",
    "from tqdm import tqdm_notebook\n",
    "import pandas as pd\n",
    "from itertools import product\n",
    "import scipy\n",
    "\n",
    "from utils.data import load_data, save_results\n",
    "from utils.models import SVM, SPR\n",
    "from utils.kernels import GaussianKernel, LinearKernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paths and Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CWD = os.getcwd()\n",
    "DATA_DIR = os.path.join(CWD, \"data\")\n",
    "RESULT_DIR = os.path.join(CWD, \"results\")\n",
    "\n",
    "FILES = {0: {\"train_mat\": \"Xtr0_mat100.csv\",\n",
    "             \"train\": \"Xtr0.csv\",\n",
    "             \"test_mat\": \"Xte0_mat100.csv\",\n",
    "             \"test\": \"Xte0.csv\",\n",
    "             \"label\": \"Ytr0.csv\"},\n",
    "         1: {\"train_mat\": \"Xtr1_mat100.csv\",\n",
    "             \"train\": \"Xtr1.csv\",\n",
    "             \"test_mat\": \"Xte1_mat100.csv\",\n",
    "             \"test\": \"Xte1.csv\",\n",
    "             \"label\": \"Ytr1.csv\"},\n",
    "         2: {\"train_mat\": \"Xtr2_mat100.csv\",\n",
    "             \"train\": \"Xtr2.csv\",\n",
    "             \"test_mat\": \"Xte2_mat100.csv\",\n",
    "             \"test\": \"Xte2.csv\",\n",
    "             \"label\": \"Ytr2.csv\"}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    with open(os.path.join(RESULT_DIR, \"results.csv\"), 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile, delimiter=',')\n",
    "\n",
    "        writer.writerow([\"Id\", \"Bound\"])\n",
    "        for i in range(3000):\n",
    "            writer.writerow([i, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment:**\n",
    "\n",
    "* We get 0.51266 which means that the dataset is pretty balanced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SPR\n",
    "\n",
    "* Simple Pattern Recognition algorithm with Gaussian kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on train set / val set 0 : 0.99875 / 0.585 (λ: 5e-05,γ: 500)\n",
      "Accuracy on train set / val set 1 : 1.0 / 0.705 (λ: 5e-05,γ: 500)\n",
      "Accuracy on train set / val set 2 : 1.0 / 0.5775 (λ: 5e-05,γ: 500)\n"
     ]
    }
   ],
   "source": [
    "γ = 500\n",
    "λ = 5e-5\n",
    "kernel = GaussianKernel(γ)\n",
    "#kernel = LinearKernel()\n",
    "\n",
    "len_files = len(FILES)\n",
    "for i in range(len_files):\n",
    "    X_train, Y_train, X_test = load_data(i, data_dir=DATA_DIR, files_dict=FILES)\n",
    "    X_val = X_train[1600:]\n",
    "    Y_val = Y_train[1600:]\n",
    "    X_train = X_train[:1600]\n",
    "    Y_train = Y_train[:1600]\n",
    "    clf = SPR(kernel)\n",
    "    clf.fit(X_train, Y_train)\n",
    "    y_pred_train =clf.predict(X_train)\n",
    "    y_pred_val = clf.predict(X_val)\n",
    "    score_train = clf.score(y_pred_train, Y_train)\n",
    "    score_val = clf.score(y_pred_val, Y_val)\n",
    "\n",
    "    print(f\"Accuracy on train set / val set {i} : {score_train} / {score_val} (λ: {λ},γ: {γ})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and test on the different sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = np.zeros(3000)\n",
    "\n",
    "for i in range(len(FILES)):\n",
    "    X_train, Y_train, X_test = load_data(i, data_dir=DATA_DIR, files_dict=FILES)\n",
    "    clf = SPR()\n",
    "    clf.fit(X_train, Y_train)\n",
    "    results[i*1000:i*1000 + 1000] = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the save results function\n",
    "save_results(\"test_results.csv\", results, result_dir=RESULT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM with Gaussian Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison with ``scikit-learn`` implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on train set / val set 0 : 1.0 / 0.575 (λ: 5e-05,γ: 500)\n",
      "Accuracy on train set / val set 1 : 1.0 / 0.7275 (λ: 5e-05,γ: 500)\n",
      "Accuracy on train set / val set 2 : 1.0 / 0.6375 (λ: 5e-05,γ: 500)\n"
     ]
    }
   ],
   "source": [
    "γ = 500\n",
    "λ = 5e-5\n",
    "kernel = GaussianKernel(γ)\n",
    "\n",
    "len_files = len(FILES)\n",
    "for i in range(len_files):\n",
    "    X_train, Y_train, X_test = load_data(i, data_dir=DATA_DIR, files_dict=FILES)\n",
    "    X_val = X_train[1600:]\n",
    "    Y_val = Y_train[1600:]\n",
    "    X_train = X_train[:1600]\n",
    "    Y_train = Y_train[:1600]\n",
    "    clf = SVM(_lambda=λ, kernel=kernel)\n",
    "    clf.fit(X_train, Y_train)\n",
    "    y_pred_train =clf.predict(X_train)\n",
    "    y_pred_val = clf.predict(X_val)\n",
    "    score_train = clf.score(y_pred_train, Y_train)\n",
    "    score_val = clf.score(y_pred_val, Y_val)\n",
    "\n",
    "    print(f\"Accuracy on train set / val set {i} : {score_train} / {score_val} (λ: {λ},γ: {γ})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C: 5.0\n"
     ]
    }
   ],
   "source": [
    "n = 2000\n",
    "print(f\"C: {1/(2 * n * λ)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on train set / val set 0 : 1.0 / 0.5725 (λ: 5e-05,γ: 500)\n",
      "Accuracy on train set / val set 1 : 1.0 / 0.705 (λ: 5e-05,γ: 500)\n",
      "Accuracy on train set / val set 2 : 1.0 / 0.5875 (λ: 5e-05,γ: 500)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "len_files = len(FILES)\n",
    "for i in range(len_files):\n",
    "    X_train, Y_train, X_test = load_data(i, data_dir=DATA_DIR, files_dict=FILES)\n",
    "    X_val = X_train[1600:]\n",
    "    Y_val = Y_train[1600:]\n",
    "    X_train = X_train[:1600]\n",
    "    Y_train = Y_train[:1600]\n",
    "    clf = SVC(C=5.0, kernel=\"rbf\", gamma=500)\n",
    "    clf.fit(X_train, Y_train)\n",
    "    y_pred_train = clf.predict(X_train)\n",
    "    y_pred_val = clf.predict(X_val)\n",
    "    score_train = np.sum([Y_train == y_pred_train]) / len(Y_train)\n",
    "    score_val = np.sum([Y_val == y_pred_val]) / len(Y_val)\n",
    "\n",
    "    print(f\"Accuracy on train set / val set {i} : {score_train} / {score_val} (λ: {λ},γ: {γ})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate and improve Kmeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Diagnostic: introduce a tolerance parameter ``tol``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(dataset_idx, clf, k=5, embeddings=None):\n",
    "    \"\"\"\n",
    "    Perform a k-fold cross-validation on a specific dataset\n",
    "    given a specific classifier\n",
    "    \n",
    "    \n",
    "    Parameters\n",
    "    -------------\n",
    "    - dataset_idx : int\n",
    "        Dataset index to be called in the data loader\n",
    "        \n",
    "    - clf : object\n",
    "        Classifier object with methods:\n",
    "        . fit\n",
    "        . predict\n",
    "        . score\n",
    "        \n",
    "    - k : int (optional)\n",
    "        Number of folds\n",
    "        Default: 5\n",
    "        \n",
    "    - embeddings : str (optional)\n",
    "        pre-computed embeddings filename\n",
    "    \n",
    "    Returns\n",
    "    -----------\n",
    "    - results : dictionary\n",
    "        Summary of the results\n",
    "        Note: the results can be display using\n",
    "        a pandas.DataFrame such as in:\n",
    "        ``pd.DataFrame(results)``\n",
    "    \"\"\"\n",
    "    \n",
    "    # Setup\n",
    "    scores_val = list()\n",
    "    scores_train = list()\n",
    "    \n",
    "    # Load data\n",
    "    X_train, Y_train, X_test = load_data(dataset_idx, data_dir=DATA_DIR, files_dict=FILES)\n",
    "    \n",
    "    if embeddings is not None:\n",
    "        X_train = np.load(embeddings)\n",
    "    \n",
    "    n = len(X_train)\n",
    "    assert n == len(Y_train)\n",
    "    \n",
    "    # Divise the samples\n",
    "    bounds = [(i * (n // k), (i+1) * (n // k))\n",
    "              for i in range(k)]\n",
    "\n",
    "\n",
    "    # Loop through the divided samples\n",
    "    for bound in bounds:\n",
    "        lower, upper = bound\n",
    "        # Create index array for validation set\n",
    "        idx = np.arange(lower, upper)\n",
    "        not_idx = [i for i in range(n) if i not in idx]\n",
    "\n",
    "        # Populate current train and val sets\n",
    "        _X_val = X_train[idx]\n",
    "        _Y_val = Y_train[idx]\n",
    "        _X_train = X_train[not_idx]\n",
    "        _Y_train = Y_train[not_idx]\n",
    "\n",
    "        # Sanity checks\n",
    "        assert len(_X_train) == len(_Y_train)\n",
    "        assert len(_X_val) == len(_Y_val)\n",
    "        assert len(_X_train) == n - len(X_train) // k\n",
    "\n",
    "        # Fit the classifier on the current training set\n",
    "        clf.fit(_X_train, _Y_train)\n",
    "        # Compute the score\n",
    "        y_pred_train = clf.predict(_X_train)\n",
    "        y_pred_val = clf.predict(_X_val)\n",
    "        score_train = clf.score(y_pred_train, _Y_train)\n",
    "        score_val = clf.score(y_pred_val, _Y_val)\n",
    "\n",
    "        scores_val.append(score_val)\n",
    "        scores_train.append(score_train)\n",
    "\n",
    "\n",
    "   \n",
    "    # Format the results in a dictionary\n",
    "    # Compute the score average and standard deviation\n",
    "    results = {\"train_scores\": scores_train,\n",
    "               \"val_scores\": scores_val,\n",
    "               \"train_avg\": np.mean(scores_train),\n",
    "               \"val_avg\": np.mean(scores_val),\n",
    "               \"train_std\": np.std(scores_train),\n",
    "               \"val_std\": np.std(scores_val)}\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_scores': [1.0, 1.0, 1.0, 1.0, 1.0],\n",
       " 'val_scores': [0.5625, 0.5675, 0.575, 0.6, 0.575],\n",
       " 'train_avg': 1.0,\n",
       " 'val_avg': 0.576,\n",
       " 'train_std': 0.0,\n",
       " 'val_std': 0.012903487900563932}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = SVM(_lambda=λ, kernel=kernel)\n",
    "\n",
    "cross_validation(0, clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of Grid Search using Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on train set / val set 0 : 1.0 / 0.55 (λ: 5e-07,γ: 50.0)\n",
      "Accuracy on train set / val set 1 : 1.0 / 0.68 (λ: 5e-07,γ: 50.0)\n",
      "Accuracy on train set / val set 2 : 1.0 / 0.615 (λ: 5e-07,γ: 50.0)\n",
      "\n",
      "\n",
      "Accuracy on train set / val set 0 : 1.0 / 0.55 (λ: 2.875e-06,γ: 50.0)\n",
      "Accuracy on train set / val set 1 : 1.0 / 0.68 (λ: 2.875e-06,γ: 50.0)\n",
      "Accuracy on train set / val set 2 : 1.0 / 0.615 (λ: 2.875e-06,γ: 50.0)\n",
      "\n",
      "\n",
      "Accuracy on train set / val set 0 : 1.0 / 0.551 (λ: 5.2500000000000006e-06,γ: 50.0)\n",
      "Accuracy on train set / val set 1 : 1.0 / 0.68 (λ: 5.2500000000000006e-06,γ: 50.0)\n",
      "Accuracy on train set / val set 2 : 1.0 / 0.615 (λ: 5.2500000000000006e-06,γ: 50.0)\n",
      "\n",
      "\n",
      "Accuracy on train set / val set 0 : 1.0 / 0.551 (λ: 7.625000000000001e-06,γ: 50.0)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-91be696d9abd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# cross validation (default: k=5)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_validation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mscore_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train_avg\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mscore_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"val_avg\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-69-89068fdd1196>\u001b[0m in \u001b[0;36mcross_validation\u001b[0;34m(dataset_idx, clf, k)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;31m# Compute the score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0my_pred_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_X_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0my_pred_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_X_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0mscore_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Y_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mscore_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Y_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/MVA/Cours_S2/KMML/KMML_challenge/utils/models.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     89\u001b[0m         \"\"\"\n\u001b[1;32m     90\u001b[0m         \u001b[0;31m# Compute the similarity matrix for faster prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0mS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_similarity_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/MVA/Cours_S2/KMML/KMML_challenge/utils/kernels.py\u001b[0m in \u001b[0;36mcompute_similarity_matrix\u001b[0;34m(self, Z, X)\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0mS0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mZ2\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mX2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mZ\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;31m# Compute the similarity matrix using numpy vectorized functions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0mS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "γ = 350\n",
    "λ = 1e-5\n",
    "gamma_list = np.linspace(50,γ,5, endpoint = True)\n",
    "lambda_list = np.linspace(5e-7, λ, 5, endpoint = True)\n",
    "\n",
    "settings = list(product(gamma_list,lambda_list))\n",
    "\n",
    "best_score = {i: 0 for i in range(3)}\n",
    "best_lambda = {i: 0 for i in range(3)}\n",
    "best_gamma = {i: 0 for i in range(3)}\n",
    "\n",
    "for k, tup in enumerate(settings):\n",
    "    \n",
    "    γ, λ = tup\n",
    "    kernel = GaussianKernel(γ)\n",
    "    \n",
    "    len_files = len(FILES)\n",
    "    clf = SVM(_lambda=λ, kernel=kernel)\n",
    "    \n",
    "    for i in range(len_files):\n",
    "        # cross validation (default: k=5)\n",
    "        results = cross_validation(i, clf)\n",
    "        score_train = results[\"train_avg\"]\n",
    "        score_val = results[\"val_avg\"]\n",
    "        print(f\"Accuracy on train set / val set {i} : {round(score_train, 3)} / {round(score_val, 3)} (λ: {λ},γ: {γ})\")\n",
    "        \n",
    "        if score_val > best_score[i]:\n",
    "            best_score[i] = score_val\n",
    "            best_lambda[i] = λ\n",
    "            best_gamma[i] = γ\n",
    "        \n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-padding implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENCODING = {'A': [1.,0.,0.,0.],\n",
    "            'C': [0.,1.,0.,0.],\n",
    "            'G': [0.,0.,1.,0.],\n",
    "            'T': [0.,0.,0.,1.],\n",
    "            'Z': [0.,0.,0.,0.]} # used in zero-padding\n",
    "\n",
    "def P(i, seq, k, zero_padding=True):\n",
    "    \"\"\"\n",
    "    Compute the a k_mers at a given position in a nucleotides sequence\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    - i : int\n",
    "        Position in the sequence\n",
    "        \n",
    "    - k : int\n",
    "        Size of k-mer to be returned\n",
    "        \n",
    "    - seq : str\n",
    "        Sequence of nucleotides\n",
    "        \n",
    "    - zero_padding : boolean (optional)\n",
    "        Whether to use zero-padding on the sequence edges\n",
    "        Default: True\n",
    "        \n",
    "    Returns\n",
    "    -----------\n",
    "    - L : numpy.array\n",
    "        One-hot encoding of the string sequence\n",
    "        \n",
    "    - not_in : boolean\n",
    "        Whether the k-mer was computed on the sequence edges\n",
    "        Always set to False when using zero-padding\n",
    "    \"\"\"\n",
    "    # Setup\n",
    "    not_in = True\n",
    "    if zero_padding:\n",
    "        not_in = False\n",
    "    \n",
    "    # lower edge\n",
    "    if i-(k+1)//2 + 1 < 0:\n",
    "        # Use heading zero padding here\n",
    "        n_zeros = abs(i - (k+1) // 2 + 1)\n",
    "        k_mer_i = 'Z'*n_zeros + seq[:  i + (k+2)//2]\n",
    "    # upper edge\n",
    "    elif i + (k+2)//2 > len(seq):\n",
    "        # Use trailing zero padding here\n",
    "        n_zeros = i + (k+2) // 2 - len(seq)\n",
    "        k_mer_i = seq[i - (k+1)//2 + 1:] + 'Z'*n_zeros\n",
    "    # in the middle\n",
    "    else:\n",
    "        k_mer_i = seq[i-(k+1)//2 + 1 :  i + (k+2)//2]\n",
    "        not_in = False\n",
    "        \n",
    "    # concatenate one hot encoding\n",
    "    L = []\n",
    "    for c in k_mer_i:\n",
    "        L += ENCODING[c]\n",
    "    \n",
    "    # Sanity check\n",
    "    assert len(L) == 4 * k\n",
    "    \n",
    "    # Convert to array and return\n",
    "    return np.array(L), not_in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigate ``sklearn`` one-hot encoding implementation\n",
    "\n",
    "**Bottom-line**: Use *sparse matrix* and/or *Compressed Sparse Row* format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CKN implementation\n",
    "\n",
    "**Pseudo-code**: (Unsupervised variant)\n",
    "\n",
    "0. Input: sequences $(x_i)_{i=\\{1, \\ldots, n\\}}$, labels $(y_i)_{i=\\{1, \\ldots, n \\}}$, $k$ (window-size), $p$ (centroids number), $iter$ (KMeans iteration number), $|A|$ (vocabulary-size: here 4 nucleotides)\n",
    "1. Process the sequences $x_i$ to obtain $k$-mers.\n",
    "2. Perform K-Means to obtain $p$ centroids in $\\mathbb{R}^{|A|k}$ using $iter$ iterations\n",
    "3. Compute $K_{ZZ}^{-1/2}$ the (pseudo-)inverse square root Gram matrix defined by $[K_0(z_i, z_j)]_{(i, j) \\in \\{1, \\ldots, p\\}^2}$\n",
    "4. Compute for each training sequence $x$ its mapping $\\phi(x) = \\frac{1}{m} \\sum_{i=1}^m \\phi_0(P_i(x)) = \\frac{1}{m} \\sum_{i=1}^m K_{ZZ}^{-1/2} K_Z(P_i(x))$ where $K_Z(P_i(x)) = [K_0(z_1, P_i(x)), \\ldots, K_0(z_p, P_i(x)]^T$\n",
    "5. Solve $$w^* = \\arg \\min_{w \\in \\mathbb{R}^p} \\frac{1}{n} \\sum_{i=1}^n L(y_i, \\langle w, \\phi(x_i) \\rangle_{\\mathbb{R}^p}) + \\lambda \\parallel w \\parallel^2$$\n",
    "Note that when $L$ is the squared loss, the optimization problems boils down to a Least Square Problem with a $l_2$ regularization, i.e. a Ridge regression problem.\n",
    "6. Compute for each testing sequence $x'$ its mapping $\\phi(x') = \\frac{1}{m} \\sum_{i=1}^m K_{ZZ}^{-1/2} K_Z(P_i(x'))$\n",
    "7. Use the $\\text{sign}(\\langle w^*, \\phi(x')\\rangle)$ criterion to assign a label to each testing sequence $x'$.\n",
    "\n",
    "Note: Since the number of k-mers can grow large, it is advisable to use a ``MiniBatch`` K-Means variant. In addition, using the ``kmeans_++`` initialization may be favorable to obtain better results. \n",
    "\n",
    "Note: The paper authors mention *extracting a large number of k-mers* which is still quite subjective. We have at most $|A|^k$ unique $k$-mers but the number of duplicates is key to perform ``K-Means`` clustering. \n",
    "\n",
    "Note: Granted we use ``zero-padding``, since for each dataset we have 2000 sequences that are all 101 characters long, we can extract at most 202000 $k$-mers per dataset which is huge. However this is still less than $|A|^k$ when $k$ is strictly larger than 8. Since the authors recommend using $k=12$, we have less $k$-mers from our training data than the number of unique $k$-mers that are possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODOs\n",
    "\n",
    "1. Process sequences to obtain $k$-mers.\n",
    "2. Process sequences to obtain a ``sparse`` $k$-mers matrix.\n",
    "3. Use ``KMeans`` on sequences (sparse VS not sparse)\n",
    "4. Use ``MiniBatchKMeans`` on sequences (sparse VS not sparse)\n",
    "5. Compute $K_{ZZ}^{-1/2}$\n",
    "6. Compute the training sequences mapping\n",
    "7. Rewrite the optimization problem into a Ridge Regression problem.\n",
    "8. Compute the testing sequences mapping\n",
    "9. Apply the ``sign`` criterion to assign a label to each testing sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences in training data: 2000\n",
      "Number of k-mers processed: 202000\n",
      "Memory size of the k-mers: 1671792 bytes\n",
      "CPU times: user 795 ms, sys: 15.2 ms, total: 810 ms\n",
      "Wall time: 808 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Process sequences to obtain k-mers\n",
    "k = 12\n",
    "\n",
    "# Load data\n",
    "X_train, Y_train, X_test = load_data(0, data_dir=DATA_DIR, files_dict=FILES, mat=False)\n",
    "\n",
    "n_seqs = len(X_train)\n",
    "print(f\"Number of sequences in training data: {n_seqs}\")\n",
    "seq_length = len(X_train[0])\n",
    "print(f\"Sequence length: {seq_length}\")\n",
    "\n",
    "# Populate the kmers list\n",
    "kmers = list()\n",
    "for seq in X_train:\n",
    "    for i in range(len(seq)):\n",
    "        kmers.append(P(i, seq, k)[0])\n",
    "\n",
    "print(f\"Number of k-mers processed: {len(kmers)}\")\n",
    "print(f\"Memory size of the k-mers: {sys.getsizeof(kmers)} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import lil_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory size of sparse k-mers: 56 bytes\n"
     ]
    }
   ],
   "source": [
    "# Convert k-mers to a sparse matrix to be used in MiniBatchKmeans\n",
    "sparse_matrix = lil_matrix(kmers)\n",
    "print(f\"Memory size of sparse k-mers: {sys.getsizeof(sparse_matrix)} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 55s, sys: 20.3 ms, total: 3min 55s\n",
      "Wall time: 3min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Use MiniBatchKMeans to compute the anchor points \n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "n_clusters = 4096\n",
    "\n",
    "# fit on the whole data\n",
    "kmeans = MiniBatchKMeans(n_clusters=n_clusters,\n",
    "        random_state=0,\n",
    "        batch_size=10000,\n",
    "        max_iter=5).fit(sparse_matrix)\n",
    "\n",
    "# Retrieve centroids\n",
    "anchors = kmeans.cluster_centers_\n",
    "print(anchors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4096, 48)\n",
      "(4096, 48)\n"
     ]
    }
   ],
   "source": [
    "# Save centroids\n",
    "np.save(\"anchors_4096_set0.npy\", anchors)\n",
    "# Load centroids\n",
    "anchors = np.load(\"anchors_4096_set0.npy\")\n",
    "print(anchors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4096, 48)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize anchors\n",
    "anchors_norm = np.linalg.norm(anchors, axis=1).reshape(-1, 1)\n",
    "normalized_anchors = anchors / anchors_norm\n",
    "normalized_anchors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.2 s, sys: 316 ms, total: 1.51 s\n",
      "Wall time: 1.22 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Compute K_ZZ in an efficient way\n",
    "σ = 0.3\n",
    "\n",
    "K_ZZ = (anchors_norm * anchors_norm.T) * np.power(np.exp(1 / σ**2), normalized_anchors.dot(normalized_anchors.T) - 1)\n",
    "K_ZZ.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 10s, sys: 45.7 s, total: 3min 56s\n",
      "Wall time: 1min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Compute K_ZZ^{-1/2} (i.e. pseudo-inverse square root)\n",
    "K_ZZ_inv_sqrt = scipy.linalg.sqrtm(np.linalg.inv(K_ZZ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment:**\n",
    "\n",
    "* I checked that using the reverse order in computation (i.e. compute the square root before performing the inversion) yields the same results: ``np.all(K_ZZ_inv_sqrt - K_ZZ_inv_sqrt_2 < 1e-8) >>> True``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
       "        0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0.]),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
       "        0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0.]),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "        0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
       "        1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0.]),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,\n",
       "        0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.,\n",
       "        1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0.]),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0.,\n",
       "        0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0.,\n",
       "        0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1.])]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Compute the training points mapping efficiently\n",
    "\n",
    "train_kmer = list()\n",
    "for seq in X_train[:1]:\n",
    "    for i in range(len(seq)):\n",
    "        train_kmer.append(P(i, seq, k)[0])\n",
    "train_kmer[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4096,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.00177207, 0.00136875, 0.00700164, ..., 0.00341937, 0.00153039,\n",
       "       0.01290227])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K_Z = np.zeros(len(anchors))\n",
    "print(K_Z.shape)\n",
    "for l in range(len(anchors)):\n",
    "    K_Z[l]  = np.linalg.norm(anchors[l]) * np.sqrt(k) * np.power(np.exp(1 / σ**2), normalized_anchors[l].dot(train_kmer[0] / np.sqrt(k)) - 1)\n",
    "K_Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00177207, 0.00136875, 0.00700164, ..., 0.00341937, 0.00153039,\n",
       "       0.01290227])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For a single k-mer, we get\n",
    "K_Z_2 = (anchors_norm.squeeze() * np.sqrt(k)) * np.power(np.exp(1 / σ**2), normalized_anchors.dot(train_kmer[0] / np.sqrt(k))  - 1)\n",
    "K_Z_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4096,)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K_ZZ_inv_sqrt.dot(K_Z_2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4096, 101)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K_Z_2 = (anchors_norm.squeeze() * np.sqrt(k)).reshape(-1, 1) * np.power(np.exp(1 / σ**2), normalized_anchors.dot(np.array(train_kmer).T / np.sqrt(k))  - 1)\n",
    "K_Z_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4096,)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_K_Z = np.mean(K_Z_2, axis=1)\n",
    "mean_K_Z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4096,)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ϕ_x0 = K_ZZ_inv_sqrt.dot(mean_K_Z)\n",
    "ϕ_x0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_kmer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 14s, sys: 2min 9s, total: 5min 24s\n",
      "Wall time: 1min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Compute the training point mapping\n",
    "\n",
    "train_embeddings = np.zeros((n_seqs, n_clusters))\n",
    "\n",
    "for x, seq in enumerate(X_train):\n",
    "    curr_kmer = list()\n",
    "    for i in range(len(seq)):\n",
    "        curr_kmer.append(P(i, seq, k)[0])\n",
    "    curr_kmer = np.array(curr_kmer)\n",
    "    K_Z = ((anchors_norm.squeeze() * np.sqrt(k)).reshape(-1, 1)\n",
    "           * np.power(np.exp(1 / σ**2), \n",
    "                      normalized_anchors.dot(curr_kmer.T / np.sqrt(k)) - 1))\n",
    "    mean_K_Z = np.mean(K_Z, axis=1)\n",
    "    train_embeddings[x] = K_ZZ_inv_sqrt.dot(mean_K_Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 4096)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save train embeddings\n",
    "np.save(\"train_embeddings_4096_set0.npy\", train_embeddings)\n",
    "# Load train embeddings\n",
    "train_embeddings = np.load(\"train_embeddings_4096_set0.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 4096)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use ``scikit-learn`` logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/inzouzouwetrust/miniconda3/envs/km/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7645"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use ``scikit-learn`` logistic regression and play around with it\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(random_state=0, solver=\"sag\", C=100).fit(train_embeddings, Y_train)\n",
    "\n",
    "clf.score(train_embeddings, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_scores': [0.92875, 0.93125, 0.925, 0.933125, 0.93],\n",
       " 'val_scores': [0.605, 0.6025, 0.615, 0.6, 0.63],\n",
       " 'train_avg': 0.929625,\n",
       " 'val_avg': 0.6104999999999999,\n",
       " 'train_std': 0.0027271780286589153,\n",
       " 'val_std': 0.011000000000000001}"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression(random_state=0, solver=\"lbfgs\", C=1000, max_iter=2000)\n",
    "\n",
    "k_split = 5\n",
    "\n",
    "# Setup\n",
    "scores_val = list()\n",
    "scores_train = list()\n",
    "\n",
    "# Load data\n",
    "X_train, Y_train, X_test = load_data(0, data_dir=DATA_DIR, files_dict=FILES)\n",
    "X_train = train_embeddings\n",
    "\n",
    "n = len(X_train)\n",
    "assert n == len(Y_train)\n",
    "\n",
    "# Divise the samples\n",
    "bounds = [(i * (n // k_split), (i+1) * (n // k_split))\n",
    "          for i in range(k_split)]\n",
    "\n",
    "\n",
    "# Loop through the divided samples\n",
    "for bound in bounds:\n",
    "    lower, upper = bound\n",
    "    # Create index array for validation set\n",
    "    idx = np.arange(lower, upper)\n",
    "    not_idx = [i for i in range(n) if i not in idx]\n",
    "\n",
    "    # Populate current train and val sets\n",
    "    _X_val = X_train[idx]\n",
    "    _Y_val = Y_train[idx]\n",
    "    _X_train = X_train[not_idx]\n",
    "    _Y_train = Y_train[not_idx]\n",
    "\n",
    "    # Sanity checks\n",
    "    assert len(_X_train) == len(_Y_train)\n",
    "    assert len(_X_val) == len(_Y_val)\n",
    "    assert len(_X_train) == n - len(X_train) // k_split\n",
    "\n",
    "    # Fit the classifier on the current training set\n",
    "    clf.fit(_X_train, _Y_train)\n",
    "    # Compute the score\n",
    "    y_pred_train = clf.predict(_X_train)\n",
    "    y_pred_val = clf.predict(_X_val)\n",
    "    #score_train = clf.score(y_pred_train, _Y_train)\n",
    "    score_train = np.sum([_Y_train == y_pred_train]) / len(_Y_train)\n",
    "    #score_val = clf.score(y_pred_val, _Y_val)\n",
    "    score_val = np.sum([_Y_val == y_pred_val]) / len(_Y_val) \n",
    "    \n",
    "    scores_val.append(score_val)\n",
    "    scores_train.append(score_train)\n",
    "\n",
    "\n",
    "\n",
    "# Format the results in a dictionary\n",
    "# Compute the score average and standard deviation\n",
    "results = {\"train_scores\": scores_train,\n",
    "           \"val_scores\": scores_val,\n",
    "           \"train_avg\": np.mean(scores_train),\n",
    "           \"val_avg\": np.mean(scores_val),\n",
    "           \"train_std\": np.std(scores_train),\n",
    "           \"val_std\": np.std(scores_val)}\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try SVM on new embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on train set / val set 0 : 0.905 / 0.574 (λ: 1e-07,γ: 1.0)\n",
      "\n",
      "\n",
      "Accuracy on train set / val set 0 : 0.83 / 0.616 (λ: 5.05e-06,γ: 1.0)\n",
      "\n",
      "\n",
      "Accuracy on train set / val set 0 : 0.623 / 0.543 (λ: 1e-05,γ: 1.0)\n",
      "\n",
      "\n",
      "Accuracy on train set / val set 0 : 1.0 / 0.606 (λ: 1e-07,γ: 13.25)\n",
      "\n",
      "\n",
      "Accuracy on train set / val set 0 : 0.999 / 0.609 (λ: 5.05e-06,γ: 13.25)\n",
      "\n",
      "\n",
      "Accuracy on train set / val set 0 : 0.988 / 0.609 (λ: 1e-05,γ: 13.25)\n",
      "\n",
      "\n",
      "Accuracy on train set / val set 0 : 1.0 / 0.61 (λ: 1e-07,γ: 25.5)\n",
      "\n",
      "\n",
      "Accuracy on train set / val set 0 : 1.0 / 0.61 (λ: 5.05e-06,γ: 25.5)\n",
      "\n",
      "\n",
      "Accuracy on train set / val set 0 : 1.0 / 0.613 (λ: 1e-05,γ: 25.5)\n",
      "\n",
      "\n",
      "Accuracy on train set / val set 0 : 1.0 / 0.613 (λ: 1e-07,γ: 37.75)\n",
      "\n",
      "\n",
      "Accuracy on train set / val set 0 : 1.0 / 0.613 (λ: 5.05e-06,γ: 37.75)\n",
      "\n",
      "\n",
      "Accuracy on train set / val set 0 : 1.0 / 0.615 (λ: 1e-05,γ: 37.75)\n",
      "\n",
      "\n",
      "Accuracy on train set / val set 0 : 1.0 / 0.618 (λ: 1e-07,γ: 50.0)\n",
      "\n",
      "\n",
      "Accuracy on train set / val set 0 : 1.0 / 0.618 (λ: 5.05e-06,γ: 50.0)\n",
      "\n",
      "\n",
      "Accuracy on train set / val set 0 : 1.0 / 0.618 (λ: 1e-05,γ: 50.0)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "γ = 50\n",
    "λ = 1e-5\n",
    "gamma_list = np.linspace(1,γ,5, endpoint = True)\n",
    "lambda_list = np.linspace(1e-7, λ, 3, endpoint = True)\n",
    "\n",
    "settings = list(product(gamma_list,lambda_list))\n",
    "\n",
    "best_score = {i: 0 for i in range(3)}\n",
    "best_lambda = {i: 0 for i in range(3)}\n",
    "best_gamma = {i: 0 for i in range(3)}\n",
    "\n",
    "for k, tup in enumerate(settings):\n",
    "    \n",
    "    γ, λ = tup\n",
    "    kernel = GaussianKernel(γ)\n",
    "    \n",
    "    len_files = len(FILES)\n",
    "    clf = SVM(_lambda=λ, kernel=kernel)\n",
    "    \n",
    "    #for i in range(len_files):\n",
    "    for i in range(1):\n",
    "        # cross validation (default: k=5)\n",
    "        results = cross_validation(i, clf, embeddings=\"train_embeddings_4096_set0.npy\")\n",
    "        score_train = results[\"train_avg\"]\n",
    "        score_val = results[\"val_avg\"]\n",
    "        print(f\"Accuracy on train set / val set {i} : {round(score_train, 3)} / {round(score_val, 3)} (λ: {λ},γ: {γ})\")\n",
    "        \n",
    "        if score_val > best_score[i]:\n",
    "            best_score[i] = score_val\n",
    "            best_lambda[i] = λ\n",
    "            best_gamma[i] = γ\n",
    "        \n",
    "    print('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "km",
   "language": "python",
   "name": "km"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
